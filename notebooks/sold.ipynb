{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2e04177-e7c2-49b9-bff3-7157fc7ba158",
   "metadata": {},
   "source": [
    "#### Sold Property Links Scrape (Non Texas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d71bc940-7d6d-4ef4-8043-4a1db77cd1ca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "from datetime import datetime\n",
    "import zipwiseproperties\n",
    "import os\n",
    "import logging\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "# Configure logging to write to a file and to the console\n",
    "log_filename = f'logger/sold_property_scraper_{datetime.now().strftime(\"%Y-%m-%d\")}.log'\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler(log_filename),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "\n",
    "def save_links_to_csv(links, filename):\n",
    "    \"\"\"\n",
    "    Save a list of links to a CSV file.\n",
    "\n",
    "    Parameters:\n",
    "    links (list): List of property links.\n",
    "    filename (str): The name of the CSV file to save the links to.\n",
    "    \"\"\"\n",
    "    with open(filename, 'a', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        for link in links:\n",
    "            writer.writerow([link])\n",
    "\n",
    "def scrape_properties(zip_code, base_url):\n",
    "    \"\"\"\n",
    "    Scrape property links for a given zip code from the base URL.\n",
    "\n",
    "    Parameters:\n",
    "    zip_code (str): The zip code to scrape properties for.\n",
    "    base_url (str): The base URL to start scraping from.\n",
    "\n",
    "    Returns:\n",
    "    list: A list of unique property links for the zipcode.\n",
    "    \"\"\"\n",
    "    all_links = []\n",
    "    try:\n",
    "        html, error = zipwiseproperties.fetch_html_with_zyte(base_url)\n",
    "        if not html:\n",
    "            logging.error(f\"Failed to fetch HTML for {zip_code}\")\n",
    "            return all_links\n",
    "\n",
    "        # Parse initial page\n",
    "        links = zipwiseproperties.parse_properties(html)\n",
    "        if links:\n",
    "            all_links.extend(links)\n",
    "            logging.info(f\"Added {len(links)} links from base URL\")\n",
    "\n",
    "        total_pages_count = zipwiseproperties.total_pages(html)\n",
    "        print(f\"{zip_code} has {total_pages_count} total pages\")\n",
    "        logging.info(f\"{zip_code} has {total_pages_count} total pages\")\n",
    "\n",
    "        # If there are multiple pages, scrape them\n",
    "        if total_pages_count >= 20:\n",
    "            price_filters = [\n",
    "                {\"min\": 0, \"max\": 200000},\n",
    "                {\"min\": 200000, \"max\": 250000},\n",
    "                {\"min\": 250000, \"max\": 300000},\n",
    "                {\"min\": 300000, \"max\": 325000},\n",
    "                {\"min\": 325000, \"max\": 350000},\n",
    "                {\"min\": 350000, \"max\": 375000},\n",
    "                {\"min\": 375000, \"max\": 400000},\n",
    "                {\"min\": 400000, \"max\": 425000},\n",
    "                {\"min\": 425000, \"max\": 450000},\n",
    "                {\"min\": 450000, \"max\": 475000},\n",
    "                {\"min\": 475000, \"max\": 500000},\n",
    "                {\"min\": 500000, \"max\": 550000},\n",
    "                {\"min\": 550000, \"max\": 600000},\n",
    "                {\"min\": 600000, \"max\": 800000},\n",
    "                {\"min\": 800000, \"max\": None}  # 800000+ (no max filter needed)\n",
    "            ]\n",
    "            bed_filters = [\n",
    "                (0, 0),  # Studio\n",
    "                (1, 1),  # 1 Bed\n",
    "                (2, 2),  # 2 Beds\n",
    "                (3, 3),  # 3 Beds\n",
    "                (4, 4),  # 4 Beds\n",
    "                (5, None)  # 5+ Beds (no max filter needed)\n",
    "            ]\n",
    "\n",
    "            for price_filter in price_filters:\n",
    "                filtered_url = zipwiseproperties.update_url_with_price(base_url, price_filter['min'], price_filter['max'])\n",
    "                # print(read_url(filtered_url))\n",
    "\n",
    "                logging.info(f\"Adding links from price filter {price_filter['min']}-{price_filter['max']}\")\n",
    "                for min_beds, max_beds in bed_filters:\n",
    "                    bed_filtered_url = zipwiseproperties.update_url_with_beds(filtered_url, min_beds, max_beds)\n",
    "                    # print(read_url(bed_filtered_url))\n",
    "                    html, error = zipwiseproperties.fetch_html_with_zyte(bed_filtered_url)\n",
    "                    pages_count = zipwiseproperties.total_pages(html)\n",
    "                    links = zipwiseproperties.parse_properties(html)\n",
    "                    if links:\n",
    "                        all_links.extend(links)\n",
    "                    logging.info(f\"Added {len(links)} links from beds filter {min_beds}-{max_beds}\")\n",
    "                    all_links.extend(scrape_all_pages(zip_code, bed_filtered_url, pages_count, min_beds, max_beds))\n",
    "        else:\n",
    "            all_links.extend(scrape_all_pages(zip_code, base_url, total_pages_count))\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error processing {zip_code}: {e}\")\n",
    "    # Remove duplicates before returning\n",
    "    unique_links = list(set(all_links))\n",
    "    logging.info(f\"Total unique links scraped for {zip_code}: {len(unique_links)}\")\n",
    "    \n",
    "    return unique_links\n",
    "\n",
    "def scrape_all_pages(zip_code, base_url, total_pages_count, min_beds=None, max_beds=None):\n",
    "    \"\"\"\n",
    "    Scrape all pages for a given zip code and URL.\n",
    "\n",
    "    Parameters:\n",
    "    zip_code (str): The zip code to scrape properties for.\n",
    "    base_url (str): The base URL to start scraping from.\n",
    "    total_pages_count (int): The total number of pages to scrape.\n",
    "    min_beds (int, optional): Minimum number of beds for filtering.\n",
    "    max_beds (int, optional): Maximum number of beds for filtering.\n",
    "\n",
    "    Returns:\n",
    "    list: A list of property links.\n",
    "    \"\"\"\n",
    "    links = []\n",
    "    for page_number in range(2, total_pages_count + 1):\n",
    "        url = zipwiseproperties.update_url_with_page(base_url, page_number)\n",
    "        html, error = zipwiseproperties.fetch_html_with_zyte(url)\n",
    "        if html:\n",
    "            page_links = zipwiseproperties.parse_properties(html)\n",
    "            if page_links:\n",
    "                links.extend(page_links)\n",
    "                logging.info(f\"Added {len(page_links)} links from page {page_number} \"\n",
    "                             f\"for {zip_code} with bed filter ({min_beds}-{max_beds})\")\n",
    "    return links\n",
    "    \n",
    "\n",
    "def process_zip_code(zip_code, filename):\n",
    "    \"\"\"\n",
    "    Process a single zip code and save property links to a CSV file.\n",
    "\n",
    "    Parameters:\n",
    "    zip_code (str): The zip code to process.\n",
    "    filename (str): The name of the CSV file to save the links to.\n",
    "    \"\"\"\n",
    "    logging.info(f\"Processing zip code: {zip_code}\")\n",
    "    base_url = zipwiseproperties.generate_zipcode_url_sold(zip_code)\n",
    "    links = scrape_properties(zip_code, base_url)\n",
    "    if links:\n",
    "        save_links_to_csv(links, filename)\n",
    "        logging.info(f\"Saved {len(links)} links for zip code: {zip_code}\")\n",
    "\n",
    "def process_msa(msa_name, msa_zipcodes, filename):\n",
    "    \"\"\"\n",
    "    Process all zip codes in an MSA and save property links to a CSV file.\n",
    "\n",
    "    Parameters:\n",
    "    msa_name (str): The name of the MSA.\n",
    "    msa_zipcodes (list): List of zip codes in the MSA.\n",
    "    filename (str): The name of the CSV file to save the links to.\n",
    "    \"\"\"\n",
    "    # Ensure directory exists\n",
    "    os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
    "    \n",
    "    with open(filename, 'w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(['Property Link'])\n",
    "\n",
    "    # Process zip codes in parallel\n",
    "    with ThreadPoolExecutor(max_workers=14) as executor:\n",
    "        futures = [executor.submit(process_zip_code, zip_code, filename) for zip_code in msa_zipcodes]\n",
    "        for future in as_completed(futures):\n",
    "            try:\n",
    "                future.result()  # Wait for each future to complete\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error processing zip code {futures[future]}: {e}\")\n",
    "\n",
    "    # Remove duplicates\n",
    "    df_links = pd.read_csv(filename)\n",
    "    df_links.drop_duplicates(inplace=True)\n",
    "    df_links.to_csv(filename, index=False)\n",
    "    logging.info(f\"Removed duplicates. Total unique links saved: {len(df_links)}\")\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to read zip codes from a CSV file and scrape property links for specified MSAs.\n",
    "    \"\"\"\n",
    "    zipcodes_df = pd.read_csv('zip_codes_by_msa.csv', dtype={'GEOID_ZCTA5_20': str})\n",
    "    zipcodes_df['GEOID_ZCTA5_20'] = zipcodes_df['GEOID_ZCTA5_20'].str.zfill(5)\n",
    "\n",
    "    msas = [\n",
    "        \"Boston-Cambridge-Newton, MA-NH\"\n",
    "        ]\n",
    "    \n",
    "    today = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "    for msa in msas:\n",
    "        print(msa)\n",
    "        filename = f'sold_properties_links/{today}/{msa.replace(\",\", \"\").replace(\"-\", \" \").lower()}_properties.csv'\n",
    "        msa_zipcodes = zipcodes_df[zipcodes_df['CBSA Title_x'] == msa]['GEOID_ZCTA5_20'].unique()\n",
    "        logging.info(f\"Starting scraping for {msa}\")\n",
    "        start_time = datetime.now()\n",
    "        process_msa(msa, msa_zipcodes, filename)\n",
    "        end_time = datetime.now()\n",
    "        logging.info(f\"Time taken for scraping {msa}: {end_time - start_time}\")\n",
    "\n",
    "# Execute\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28e8a297-0d32-48ca-bd51-676ec0af3624",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "##### JSON File Extracter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d61f77ae-7df1-47d5-bb79-e29f80e3b7f6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import csv\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "import logging\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import zipwiseproperties\n",
    "from queue import Queue\n",
    "import os\n",
    "\n",
    "# Configure logging\n",
    "log_filename = f'logger/json_scraper_{datetime.now().strftime(\"%Y-%m-%d\")}.log'\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler(log_filename),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "\n",
    "BATCH_SIZE = 500  # Write to CSV every 100 URLs processed\n",
    "\n",
    "\n",
    "def create_directory(city):\n",
    "    today = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "    # base_directory = os.getcwd()\n",
    "    base_directory = '/scratch/kota.sha'  # Set the base directory to the desired location\n",
    "    directory = os.path.join(base_directory, 'sold_properties_results', city, today)\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "    return directory\n",
    "  \n",
    "def process_url(url, sl_no):\n",
    "    try:\n",
    "        html, error = zipwiseproperties.fetch_html_with_zyte(url)\n",
    "        if not html:\n",
    "            return url, error  # Return the error for logging\n",
    "\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        data_script = soup.find('script', id='__NEXT_DATA__')\n",
    "        data_json = json.loads(data_script.string)\n",
    "        sold_data = json.loads(data_json['props']['pageProps']['componentProps']['gdpClientCache'])\n",
    "        \n",
    "        first_key = next(iter(sold_data))\n",
    "        first_element_value = sold_data[first_key]\n",
    "\n",
    "        # Convert the first element into JSON format\n",
    "        json_data = json.dumps(first_element_value, indent=4)\n",
    "        \n",
    "        return sl_no, url, json_data  # Return the data for saving\n",
    "\n",
    "    except Exception as e:\n",
    "        return url, str(e)  # Return error message\n",
    "\n",
    "def flush_to_csv(queue, file_path, headers):\n",
    "    \"\"\"\n",
    "    Write a batch of results or errors to the CSV file.\n",
    "    \"\"\"\n",
    "    with open(file_path, 'a', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        while not queue.empty():\n",
    "            writer.writerow(queue.get())\n",
    "\n",
    "def scrape_city(city, csv_filename, max_workers):\n",
    "    directory = create_directory(city)\n",
    "    json_dir = os.path.join(directory, 'jsonfiles')\n",
    "    os.makedirs(json_dir, exist_ok=True)\n",
    "\n",
    "    results_file = f'{directory}/{city}_sold_property_details.csv'\n",
    "    error_file = f'{directory}/error_sold_property_urls.csv'\n",
    "\n",
    "    start_time = datetime.now()\n",
    "    logging.info(f\"Scraping started at: {start_time} for {city}\")\n",
    "\n",
    "    # Load property links into a Pandas DataFrame for easier manipulation\n",
    "    df = pd.read_csv(csv_filename)\n",
    "    property_links = df.iloc[:, 0].tolist()  # Assuming the first column contains URLs\n",
    "\n",
    "    logging.info(f\"Total URLs to process: {len(property_links)}\")\n",
    "\n",
    "    # Load already processed URLs and their serial numbers from the results file\n",
    "    processed_urls = set()\n",
    "    processed_sl_nos = set()  # To track processed serial numbers\n",
    "    last_sl_no = 0\n",
    "    if os.path.exists(results_file):\n",
    "        processed_df = pd.read_csv(results_file)\n",
    "        processed_urls = set(processed_df['URL'].tolist())  # Assuming 'URL' is the column name\n",
    "        processed_sl_nos = set(processed_df['Serial Number'].tolist())  # Track serial numbers\n",
    "        last_sl_no = max(processed_sl_nos) if processed_sl_nos else 0\n",
    "        logging.info(f\"Found {len(processed_urls)} already processed URLs. Skipping them.\")\n",
    "\n",
    "    # Filter out already processed URLs\n",
    "    remaining_urls = [(sl_no + 1, url) for sl_no, url in enumerate(property_links) if url not in processed_urls]\n",
    "    logging.info(f\"Remaining URLs to process: {len(remaining_urls)}\")\n",
    "\n",
    "    success_queue = Queue()  # Thread-safe queues for collecting success and error results\n",
    "    error_queue = Queue()\n",
    "\n",
    "    # Prepare the results CSV headers if the results file does not exist\n",
    "    if not os.path.exists(results_file):\n",
    "        with open(results_file, 'w', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow(['Serial Number', 'URL', 'JSON File Path'])\n",
    "\n",
    "    if not os.path.exists(error_file):\n",
    "        with open(error_file, 'w', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow(['URL', 'Error'])\n",
    "\n",
    "    processed_count = 0\n",
    "\n",
    "    # Use ThreadPoolExecutor for concurrent URL processing\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        future_to_url = {executor.submit(process_url, url, sl_no): (sl_no, url) for sl_no, url in remaining_urls}\n",
    "        for future in as_completed(future_to_url):\n",
    "            result = future.result()\n",
    "            processed_count += 1\n",
    "\n",
    "            if len(result) == 3:\n",
    "                # Success: Write the JSON data to a file\n",
    "                sl_no, url, json_data = result\n",
    "                json_file_path = os.path.join(json_dir, f'{sl_no}.json')  # JSON file named after sl_no\n",
    "                with open(json_file_path, 'w') as json_file:\n",
    "                    json_file.write(json_data)\n",
    "                success_queue.put((sl_no, url, json_file_path))  # Add to success queue\n",
    "            else:\n",
    "                # Error: Log the error URL\n",
    "                url, error = result\n",
    "                error_queue.put((url, error))  # Add to error queue\n",
    "\n",
    "            # Every BATCH_SIZE URLs, write results and errors to CSV\n",
    "            if processed_count % BATCH_SIZE == 0:\n",
    "                flush_to_csv(success_queue, results_file, ['Serial Number', 'URL', 'JSON File Path'])\n",
    "                flush_to_csv(error_queue, error_file, ['URL', 'Error'])\n",
    "                logging.info(f\"Processed {processed_count} URLs so far...\")\n",
    "\n",
    "    # Final flush for any remaining results or errors\n",
    "    flush_to_csv(success_queue, results_file, ['Serial Number', 'URL', 'JSON File Path'])\n",
    "    flush_to_csv(error_queue, error_file, ['URL', 'Error'])\n",
    "\n",
    "    end_time = datetime.now()\n",
    "    duration = end_time - start_time\n",
    "\n",
    "    logging.info(f\"All remaining property links processed for {city}. Time taken for scraping: {duration.total_seconds()} seconds\")\n",
    "    logging.info(f\"All property details saved to {results_file}\")\n",
    "    logging.info(f\"URLs with errors saved to {error_file}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    logging.info(\"Scraping session started\")\n",
    "    total_start_time = datetime.now()\n",
    "\n",
    "    cities_info = [\n",
    "        (\"boston\",\"sold_properties_links/2024-09-04/boston cambridge newton ma nh_properties.csv\",14)\n",
    "    ]\n",
    "\n",
    "    for city, csv_filename, max_workers in cities_info:\n",
    "        scrape_city(city, csv_filename, max_workers)\n",
    "\n",
    "    total_end_time = datetime.now()\n",
    "    total_duration = total_end_time - total_start_time\n",
    "    logging.info(f\"Total scraping session completed in {total_duration.total_seconds()} seconds\")\n",
    "\n",
    "     "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
